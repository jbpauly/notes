{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis\n",
    "### PyCon 2017\n",
    "### Author: Aileen Nielsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Correlation\n",
    "\n",
    "## Autocorrelation Function\n",
    "- Used to help identify possible structures of time series data\n",
    "- Gives a sense of how different points in time relate to each other in a way explained by temporal distance\n",
    "- Need to have a standard frequency of data, not sparse\n",
    "### Problem that can arise\n",
    "- Does not account for periodic or seasonal data\n",
    "- Does not account for trending\n",
    "\n",
    "### ACF plotting\n",
    "- Sample 0 will always have correlation of 1 (with itself)\n",
    "- Boundaries of significance\n",
    "- **Periodic data will not have a good lag cut off**\n",
    "### Problem that can arise\n",
    "- Does not account of trending\n",
    "\n",
    "## Partial Autocorrelation Function\n",
    "- “gives the partial correlation of a time series with its own lagged values, controlling for the values of time series at all shorter lags”\n",
    "### Why would this be useful?\n",
    "- Does not recycle correlation of shorter time period, like autocorrelation does\n",
    "- better accounts for periodicity\n",
    "- **Takes out the shorter correlation**, then looks for additional correlations\n",
    "\n",
    "# Pre-Prediction Munging & Stationarity  \n",
    "- Most time series are not naturally stationary\n",
    "    - contain seasonality\n",
    "    - contrain trending\n",
    "- Standard and statistic methods require stationary data for forecasting\n",
    "\n",
    "## Stationarity\n",
    "- Need to remove the trend and seasonal elements before forecasting\n",
    "- Most data in the real world shows trends and seasonality\n",
    "- Most models require data that shows neither of these properties to say something interesting \n",
    "\n",
    "Elements of Stationarity:\n",
    "1. Constant Mean\n",
    "    - Value is not drifting over time\n",
    "2. Constant Variance\n",
    "    - Unpredictability is not changing over time\n",
    "3. Constant Autocorrelation\n",
    "    - Structure is not changing over time\n",
    "### Caveat\n",
    "- Not realistic in the real world but can be true locally\n",
    "\n",
    "### Handling non-stationarity\n",
    "#### Remove Trending\n",
    "1. Take a diff or log diff to take out trending\n",
    "2. Take a moving average (with window function) and subtract it out\n",
    "    - computationally expensive\n",
    "    - different value subtracted at every point, so difficult to explain, re-transform, etc.\n",
    "3. Take linear regression\n",
    "    - less complex than moving average, b/c only 1 variable to re-transform\n",
    "    - less effective than moving average, b/c will not always fully detrend\n",
    "        - may require a send level transform which is non-optimal\n",
    "\n",
    "#### Remove Seasonality \n",
    "Seasonality can be additive or multiplicative.\n",
    "- In the real work, mostly see multiplicative\n",
    "1. Average de-trended values for specific season\n",
    "    - **simplest**\n",
    "2. Use 'loess' method (locally weighted scatterplot smoothing)\n",
    "    - **most common**\n",
    "    - Window of specified width is placed over the data\n",
    "    - A weighted pregression line or curve is fitted to the data, with points closest to center of curve having greatest weight\n",
    "    - Weighting is reduced on points farthest from regression line/curve and calculation is rerun several times\n",
    "    - This yield one point on loess curve\n",
    "    - Helps reduce impact of outlier points \n",
    "    - **Computationally taxing**, but typically how seasonality is dealt with\n",
    "\n",
    "#### Remove increasing variance\n",
    "1. Power transformation\n",
    "2. Log transformation\n",
    "\n",
    "#### Remove autocorrelation\n",
    "**DON'T**, at least typically don't try and do this\n",
    "- it means you have fundamentally different processes and shouldn't remove them\n",
    "\n",
    "#### Most Important Factors to Consider when transforming for stationarity\n",
    "- **Try and use transformations that are 1 to 1**\n",
    "    - That way it's easy to get back to original solution space\n",
    "- It is okay to have multiple transforms if you can keep track of it, AND it is necessary\n",
    "\n",
    "### Validate Stationarity\n",
    "Standard practice is to use the Dickey-Fuller Test\n",
    "- Test the null hypothesis of whether a unit root is present in an autoregressive model\n",
    "> Y_t = ρ*Y_t-1 + u_t\n",
    "test whether ρ = 1\n",
    "- The test gives back several values to help you assess significance with standard p-value reasoning\n",
    "- **Basic Intuition: ρ having unit value means it's not stationary** \n",
    "\n",
    "\n",
    "# Forecasting\n",
    "Once we have stationarity, can move onto forecasting (at least for statistical methods)\n",
    "\n",
    "## Moving Average (MA)\n",
    "- Defined as having the form:\n",
    "> X_t = μ + ε_t + θ_1*ε_t-1 +…+ θ_q*ε_t-q\n",
    "> μ is the mean of the series\n",
    "> θ are parameters\n",
    "> θ_q not 0\n",
    "- This is a stationary process regardless of values of θ\n",
    "- Consider an MA(1) process (centered at 0):\n",
    "> X_t = ε_t + θ_1*ε_t-1\n",
    "- **Essentially MA says you oscolate around a mean, μ, with error**\n",
    "\n",
    "## Autoregressive Process (AR)\n",
    "- Defined as having the form:\n",
    "> X_t = φ_1*X_t-1 +…+ φ_pX_t-p + ε_t\n",
    "- This is a stationary process if abs(φ) < 1\n",
    "- Consider an AR(1) process:\n",
    "> X_t = φ+1*X+t-1 + ε+t\n",
    "- **Essentially AR says your past value has something to do with your present value**\n",
    "\n",
    "## ARIMA Model (AKA Box Jenkins)\n",
    "- AR = autoregressive terms\n",
    "- I = differencing\n",
    "    - **Just detrends the data**\n",
    "    - Optional \n",
    "- MA = moving average\n",
    "- Hence specified as (autoregressive terms, differencing terms, moving average terms)\n",
    "- **Note that we don't have to take out trends in our data using ARIMA, since the model can do it for us**\n",
    "    - **BUT** you have to specify the differencing\n",
    "    \n",
    "### Summary of ARIMA\n",
    "ARIMA MODE: ‘THE MOST GENERAL CLASS OF MODELS FOR FORECASTING A TIME SERIES WHICH CAN BE MADE TO BE STATIONARY\n",
    "- Statistical properties (mean, variance) constant overt time\n",
    "- ‘its short-term random time patterns always look the same in a statistical sense’\n",
    "- Autocorrelation function & power spectrum remain constant over time\n",
    "- Ok to do non-linear transformations to get there\n",
    "- ARIMA model can be viewed as a combination of signal ad noise\n",
    "- Extrapolate the signal to obtain forecasts\n",
    "\n",
    "### Applying the Appropriate ARIMA Model\n",
    "- Need to determine what ARIMA model to use\n",
    "- Use plot of the data, the ACF, and the PACF\n",
    "- With the plot of the data: look for trend (linear or otherwise) & determine whether to transform data\n",
    "- Most software will use a maximum likelihood estimation to determine appropriate ARIMA parameters\n",
    "\n",
    "#### Simplified\n",
    "1. Use PACF for AR model diagnostics\n",
    "2. Use ACF for MA model diagnostics\n",
    "\n",
    "### Drawback of ARIMA\n",
    "When thinking about the model...\n",
    "- Number of timestamps able to forecast is limited by the number of lags used\n",
    "    - X lags == X forecast duration\n",
    "        - Otherwise **ERROR WILL COMPOUND and ERROR BOUNDS WILL BE HUGE**\n",
    "    - **Do not give into just increasing the order of model, b/c can lead to overfitting**\n",
    "- Also keep in mind that you assume a stationary mean, so don't know the ground truth\n",
    "    \n",
    "### Application\n",
    "#### General Steps\n",
    "1. remove trending\n",
    "    - remember that can try accounting for it with ARIMA, I, term\n",
    "2. remove seasonality\n",
    "    - sometimes okay to proceed with seasonality just to get a feeling for the data and initial models\n",
    "3. find partial auto regressive term (used for AR)\n",
    "    - use your knoweldge of the data to interpret\n",
    "    - try and be parsimonious \n",
    "4. find auto regressive term (used for MA)\n",
    "    - use your knoweldge of the data to interpret\n",
    "    - try and be parsimonious\n",
    "\n",
    "### What if you don't take out seasonality?\n",
    "#### Use a seasonal ARIMA\n",
    "Decompose model into two components\n",
    "1. seasonal\n",
    "- has it's own AR and MA terms\n",
    "2. non seasonal\n",
    "- has it's own AR and MA terms\n",
    "\n",
    "Resource links: \n",
    "1. https://otexts.com/fpp2/seasonal-arima.html\n",
    "2. https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html\n",
    "\n",
    "#### And another model ontop of ARIMA\n",
    "\n",
    "### Confidence intervals for ARIMA\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARIMAResults.conf_int.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
