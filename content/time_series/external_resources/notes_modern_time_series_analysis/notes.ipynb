{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Time Series Analysis\n",
    "### SciPy 2019 Tutorial\n",
    "### Author: Aileen Nielsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box-Jenkins ARIMA Modeling (background)\n",
    "- Developed early to mid 20th century\n",
    "- AR, MA, etc. in same category\n",
    "- Success and remains quite close to cutting edge performance\n",
    "- Excellent performance on **small datasets**\n",
    "\n",
    "**When using modern time series analysis, always want to check that your getting more than what you can get with ARIMA type models. OCCAM'S RAZOR**\n",
    "- ARIMA is actually a pretty high bar\n",
    "\n",
    "### ARIMA formula\n",
    "A nonseasonal ARIMA model is classified as an \"ARIMA(p,d,q)\" model, where:\n",
    "- p is the number of autoregressive terms,\n",
    "- d is the number of nonseasonal differences needed for stationarity, and\n",
    "- q is the number of lagged forecast errors in the prediction equation.\n",
    "> ŷt   =   μ + ϕ1 yt-1 +…+ ϕp yt-p - θ1et-1 -…- θqet-q\n",
    "\n",
    "### What's Missing from ARIMA\n",
    "- Not especially intuitive\n",
    "- No way to build in our underlying understanding about how it works\n",
    "    - random walk element\n",
    "    - cyclical element\n",
    "    - external regressors \n",
    "- Some systems cycle more slowly or stochastically than can be easily described with an ARIMA model\n",
    "    - Good performance on small datasets\n",
    "        - 7 day forecasting with daily data\n",
    "    - Typically bad performance with large\n",
    "        - 7 day forecasting with hourly or minute data\n",
    "        - Minute data with hourly forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Space Models \n",
    "\n",
    "## Structural time series\n",
    "- Can be expressed in ARIMA form\n",
    "- Fit viam maximum likelihood/Kalman filter\n",
    "- Largely developed in econometrics\n",
    "- Offer insights into underlying structure\n",
    "- Also possible to inject Bayesian analysis via priors on parameters \n",
    "- Easier to understand than ARIMA\n",
    "    - **Can describe leveling, seasonality, and error, VISUALLY**\n",
    "- Used in original Apollo missions\n",
    "    - At each time, t, only have to update your state, not all timestamps\n",
    "\n",
    "\n",
    "### What do they offer?\n",
    "- **Filtering**: distribution of the current state at time t given all pervious measurements up to and including time t\n",
    "    - Example: Kalman filter\n",
    "- **Prediction**: the distribution of the future state at time t+k given all previous measurements up to and including time t\n",
    "- **Smoothing**: the distribution of a given state at time k given all previous and futre measurements for 0 to T (last time)\n",
    "\n",
    "### Components\n",
    "- State\n",
    "- Measurement\n",
    "- Error\n",
    "\n",
    "### Usecases\n",
    "**Need some sort of hypothesis about dynamics of your system**\n",
    "- Good use case: rocket tragectory\n",
    "    - Why? Have physics to tie hypothesis to\n",
    "- Bad use case: blanket stock forecast\n",
    "    - Why? No underlying hypothesis \n",
    "    \n",
    "### Evaluating State space\n",
    "Use AIC \n",
    "\n",
    "\n",
    "### Applying Structural Models\n",
    "Use: \n",
    "\n",
    "statsmodels ~ [UnobservedComponents](https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.structural.UnobservedComponents.html)\n",
    "\n",
    "- Allows you to plug and play with vaious structural compenents\n",
    "    - level\n",
    "    - trend\n",
    "    - seasonal\n",
    "    - cycle\n",
    "    - autoregressive\n",
    "    - and much more\n",
    "    \n",
    "    \n",
    "### When to Use structural models\n",
    "Good for exploratory data analysis\n",
    "**Be cautious to use in production**\n",
    "- ie Don't fly an airplane or treat cancer with it\n",
    "- Because so many knobs to turn\n",
    "- Similar problem with ARIMA\n",
    "\n",
    "Can use if move it to Bayesian model\n",
    "- Have stronger inputs\n",
    "- [Google API](https://github.com/dafiti/causalimpact)\n",
    "\n",
    "## Hidden Markov Models (HMMs)\n",
    "- State space model: observations are an indicator of underlying state\n",
    "- Markov process: past doesn’t matter if present status is known\n",
    "- Parameter estimation: Baum-Welch algorithm\n",
    "- Smoothing/state labeling: Viterbi algorithm\n",
    "\n",
    "#### Baum-Welch Algorithm for Determining Parameters\n",
    "- Expectation-maximization parameter estimation:\n",
    "    - Initialize parameters (with informative priors or randomly)\n",
    "    - EM iterations:\n",
    "        - Compute the expectation of the log likelihood given the data\n",
    "        - Choose the parameters that maximize the log likelihood expectation\n",
    "    - Exit when desired convergence is reached\n",
    "- Guaranteed that the likelihood increases with each iteration\n",
    "- BUT converges to a local maximum not a global maximum\n",
    "- BUT can overfit the data\n",
    "\n",
    "##### Baum-Welch Algorithm Details\n",
    "Solve for the following:\n",
    "1. a (alpha, forward) ~ transition matrix probability\n",
    "    - from X_t-1 to X_t how likely is state going to change\n",
    "2. b (beta, backward) ~ Whats prob of seeing a value at Y given a particular state at X\n",
    "    - how likely is a particular Y assuming a particular underlying state\n",
    "3. pi ~ priors\n",
    "    - how likely are you to be at a particular starting point\n",
    "\n",
    "##### Problems with Baum-Welch Algorithm or other HMM\n",
    "- Again, many knobs to turn\n",
    "- improve with every cycle but may only find local maxima\n",
    "- often times need good domain knowledge to set boundaries\n",
    "\n",
    "##### Pros of HMM\n",
    "- Can account for sudden state change, unlike ARIMA and Statebased "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Time Series\n",
    "General models applied to time series need feature generation to account for temporal aspect vs. traditional time series methods can get by with just univariate or multivariate data (no features) \n",
    "\n",
    "## Feature Generation\n",
    "### Examples\n",
    "- min\n",
    "- max\n",
    "- number peaks\n",
    "- median\n",
    "- mean\n",
    "- etc.\n",
    "### Difficulties\n",
    "- with long dataset, can get tough and computationally expensive\n",
    "- Peaks aren't always intuitive for computers\n",
    "### How To\n",
    "- well studies area\n",
    "- [Catch22 canonical set](https://link.springer.com/article/10.1007/s10618-019-00647-x) is a good guide **FOR GENERAL TIME SERIES, WITHOUT DOMAIN KNOWELDGE**\n",
    "- Can almost always do better than Catch22 with custom features if you have domain knowledge\n",
    "    - ie with EKG data, there are specific features of EKG readings\n",
    "    \n",
    "#### Checking usabilty\n",
    "- Always check whether your features are useful\n",
    "\n",
    "## Time Series Classification and Forecasting\n",
    "\n",
    "### Trees\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "### xgboost\n",
    "\n",
    "### Clustering\n",
    "\n",
    "### Time series clustering\n",
    "- Surprisingly difficult\n",
    "    - Conceptually\n",
    "    - Computational costs\n",
    "    - Pitfall: Euclidean distance\n",
    "- Used across many disciplines\n",
    "    - Medicine\n",
    "    - Finance\n",
    "    - Chemistry\n",
    "    - Etc\n",
    "    \n",
    "### Dynamic time warping\n",
    "- Computationally intense\n",
    "\n",
    "### Forcasting\n",
    "Stationarity is not a requirement for ML forecasting, like with statistical time series methods, but it can help\n",
    "\n",
    "#### Compare with Statistical and state based models\n",
    "- Don't need to use temporal aspect of data\n",
    "- Instead create features for every forecast at timestamp t\n",
    "\n",
    "#### Scoring\n",
    "With time series forecasting, no single metric will give you the full picture\n",
    "- see example from NB 3: Trees for Classification and Prediction\n",
    "    - RMSE looks equivalent for last two models, but major difference in actual fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Time Series\n",
    "No need to go into details\n",
    "## Model Options \n",
    "### RNN\n",
    "#### GRU\n",
    "#### LSTM\n",
    "### CNN\n",
    "### LSTNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Options / Outlook\n",
    "\n",
    "1. Look into automated forecasting with tech company open source libraries like Prophet\n",
    "\n",
    "2. The future is a combining machine learning and statistical approaches, so good to have the stats background "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}